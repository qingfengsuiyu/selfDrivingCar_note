{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feature_Alnet.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "9Qf-DcxuL32X",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 3
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "50c6f6fc-45e9-461f-e548-5c2bf8b5a4ec",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521545653633,
          "user_tz": -480,
          "elapsed": 7636,
          "user": {
            "displayName": "郝晓翼",
            "photoUrl": "//lh4.googleusercontent.com/-fyL4ZV1Be6g/AAAAAAAAAAI/AAAAAAAAAEw/TPv5-MRrZqA/s50-c-k-no/photo.jpg",
            "userId": "110086297311802405920"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "! wget https://d17h27t6h515a5.cloudfront.net/topher/2016/November/581faac4_traffic-signs-data/traffic-signs-data.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-03-20 11:34:07--  https://d17h27t6h515a5.cloudfront.net/topher/2016/November/581faac4_traffic-signs-data/traffic-signs-data.zip\n",
            "Resolving d17h27t6h515a5.cloudfront.net (d17h27t6h515a5.cloudfront.net)... 54.230.80.232, 54.230.80.92, 54.230.80.33, ...\n",
            "Connecting to d17h27t6h515a5.cloudfront.net (d17h27t6h515a5.cloudfront.net)|54.230.80.232|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 120740327 (115M) [application/zip]\n",
            "Saving to: ‘traffic-signs-data.zip’\n",
            "\n",
            "traffic-signs-data. 100%[===================>] 115.15M  30.6MB/s    in 4.7s    \n",
            "\n",
            "2018-03-20 11:34:12 (24.6 MB/s) - ‘traffic-signs-data.zip’ saved [120740327/120740327]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0soBuQzIO0Xe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e1d47321-aa46-41c0-90be-46bf0d43b73d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521546384347,
          "user_tz": -480,
          "elapsed": 1517,
          "user": {
            "displayName": "郝晓翼",
            "photoUrl": "//lh4.googleusercontent.com/-fyL4ZV1Be6g/AAAAAAAAAAI/AAAAAAAAAEw/TPv5-MRrZqA/s50-c-k-no/photo.jpg",
            "userId": "110086297311802405920"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bvlc_alexnet.npy\t\t  datalab  traffic-signs-data.zip\r\n",
            "CarND-Alexnet-Feature-Extraction  test.p   train.p\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kfaHFJfLP1CN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "BkYQSPHzO25C",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "net_data = np.load(\"bvlc_alexnet.npy\", encoding=\"latin1\").item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K5xwIVKmOmGn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# load weights\n",
        "net_data = np.load(\"bvlc_alexnet.npy\", encoding=\"latin1\").item()\n",
        "\n",
        "\n",
        "def conv(input, kernel, biases, k_h, k_w, c_o, s_h, s_w,  padding=\"VALID\", group=1):\n",
        "    '''\n",
        "    From https://github.com/ethereon/caffe-tensorflow\n",
        "    '''\n",
        "\n",
        "    # 判断输入和通道的数据类型\n",
        "    c_i = input.get_shape()[-1]\n",
        "    assert c_i % group == 0\n",
        "    assert c_o % group == 0\n",
        "    convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n",
        "\n",
        "    if tf.__version__ < \"1.0.0\":\n",
        "        if group == 1:\n",
        "            conv = convolve(input, kernel)\n",
        "        else:\n",
        "            input_groups = tf.split(3, group, input)\n",
        "            kernel_groups = tf.split(3, group, kernel)\n",
        "            output_groups = [convolve(i, k) for i, k in zip(input_groups, kernel_groups)]\n",
        "            conv = tf.concat(3, output_groups)\n",
        "    else:\n",
        "        if group == 1:\n",
        "            conv = convolve(input, kernel)\n",
        "        else:\n",
        "            input_groups = tf.split(input, group, 3)\n",
        "            kernel_groups = tf.split(kernel, group, 3)\n",
        "            output_groups = [convolve(i, k) for i, k in zip(input_groups, kernel_groups)]\n",
        "            conv = tf.concat(output_groups, 3)\n",
        "    return tf.reshape(tf.nn.bias_add(conv, biases), [-1] + conv.get_shape().as_list()[1:])\n",
        "\n",
        "\n",
        "def AlexNet(features, feature_extract=False):\n",
        "    \"\"\"\n",
        "    Builds an AlexNet model, loads pretrained weights\n",
        "    \"\"\"\n",
        "    # conv1\n",
        "    # conv(11, 11, 96, 4, 4, padding='VALID', name='conv1')\n",
        "    k_h = 11\n",
        "    k_w = 11\n",
        "    c_o = 96\n",
        "    s_h = 4\n",
        "    s_w = 4\n",
        "    conv1W = tf.Variable(net_data[\"conv1\"][0])\n",
        "    conv1b = tf.Variable(net_data[\"conv1\"][1])\n",
        "    conv1_in = conv(features, conv1W, conv1b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=1)\n",
        "    conv1 = tf.nn.relu(conv1_in)\n",
        "\n",
        "    # lrn1\n",
        "    # lrn(2, 2e-05, 0.75, name='norm1')\n",
        "    radius = 2\n",
        "    alpha = 2e-05\n",
        "    beta = 0.75\n",
        "    bias = 1.0\n",
        "    lrn1 = tf.nn.local_response_normalization(conv1, depth_radius=radius, alpha=alpha, beta=beta, bias=bias)\n",
        "\n",
        "    # maxpool1\n",
        "    # max_pool(3, 3, 2, 2, padding='VALID', name='pool1')\n",
        "    k_h = 3\n",
        "    k_w = 3\n",
        "    s_h = 2\n",
        "    s_w = 2\n",
        "    padding = 'VALID'\n",
        "    maxpool1 = tf.nn.max_pool(lrn1, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
        "\n",
        "    # conv2\n",
        "    # conv(5, 5, 256, 1, 1, group=2, name='conv2')\n",
        "    k_h = 5\n",
        "    k_w = 5\n",
        "    c_o = 256\n",
        "    s_h = 1\n",
        "    s_w = 1\n",
        "    group = 2\n",
        "    conv2W = tf.Variable(net_data[\"conv2\"][0])\n",
        "    conv2b = tf.Variable(net_data[\"conv2\"][1])\n",
        "    conv2_in = conv(maxpool1, conv2W, conv2b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
        "    conv2 = tf.nn.relu(conv2_in)\n",
        "\n",
        "    # lrn2\n",
        "    # lrn(2, 2e-05, 0.75, name='norm2')\n",
        "    radius = 2\n",
        "    alpha = 2e-05\n",
        "    beta = 0.75\n",
        "    bias = 1.0\n",
        "    lrn2 = tf.nn.local_response_normalization(conv2, depth_radius=radius, alpha=alpha, beta=beta, bias=bias)\n",
        "\n",
        "    # maxpool2\n",
        "    # max_pool(3, 3, 2, 2, padding='VALID', name='pool2')\n",
        "    k_h = 3\n",
        "    k_w = 3\n",
        "    s_h = 2\n",
        "    s_w = 2\n",
        "    padding = 'VALID'\n",
        "    maxpool2 = tf.nn.max_pool(lrn2, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
        "\n",
        "    # conv3\n",
        "    # conv(3, 3, 384, 1, 1, name='conv3')\n",
        "    k_h = 3\n",
        "    k_w = 3\n",
        "    c_o = 384\n",
        "    s_h = 1\n",
        "    s_w = 1\n",
        "    group = 1\n",
        "    conv3W = tf.Variable(net_data[\"conv3\"][0])\n",
        "    conv3b = tf.Variable(net_data[\"conv3\"][1])\n",
        "    conv3_in = conv(maxpool2, conv3W, conv3b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
        "    conv3 = tf.nn.relu(conv3_in)\n",
        "\n",
        "    # conv4\n",
        "    # conv(3, 3, 384, 1, 1, group=2, name='conv4')\n",
        "    k_h = 3\n",
        "    k_w = 3\n",
        "    c_o = 384\n",
        "    s_h = 1\n",
        "    s_w = 1\n",
        "    group = 2\n",
        "    conv4W = tf.Variable(net_data[\"conv4\"][0])\n",
        "    conv4b = tf.Variable(net_data[\"conv4\"][1])\n",
        "    conv4_in = conv(conv3, conv4W, conv4b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
        "    conv4 = tf.nn.relu(conv4_in)\n",
        "\n",
        "    # conv5\n",
        "    # conv(3, 3, 256, 1, 1, group=2, name='conv5')\n",
        "    k_h = 3\n",
        "    k_w = 3\n",
        "    c_o = 256\n",
        "    s_h = 1\n",
        "    s_w = 1\n",
        "    group = 2\n",
        "    conv5W = tf.Variable(net_data[\"conv5\"][0])\n",
        "    conv5b = tf.Variable(net_data[\"conv5\"][1])\n",
        "    conv5_in = conv(conv4, conv5W, conv5b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
        "    conv5 = tf.nn.relu(conv5_in)\n",
        "\n",
        "    # maxpool5\n",
        "    # max_pool(3, 3, 2, 2, padding='VALID', name='pool5')\n",
        "    k_h = 3\n",
        "    k_w = 3\n",
        "    s_h = 2\n",
        "    s_w = 2\n",
        "    padding = 'VALID'\n",
        "    maxpool5 = tf.nn.max_pool(conv5, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
        "\n",
        "    # fc6, 4096\n",
        "    fc6W = tf.Variable(net_data[\"fc6\"][0])\n",
        "    fc6b = tf.Variable(net_data[\"fc6\"][1])\n",
        "    flat5 = tf.reshape(maxpool5, [-1, int(np.prod(maxpool5.get_shape()[1:]))])\n",
        "    fc6 = tf.nn.relu(tf.matmul(flat5, fc6W) + fc6b)\n",
        "\n",
        "    # fc7, 4096\n",
        "    fc7W = tf.Variable(net_data[\"fc7\"][0])\n",
        "    fc7b = tf.Variable(net_data[\"fc7\"][1])\n",
        "    fc7 = tf.nn.relu(tf.matmul(fc6, fc7W) + fc7b)\n",
        "\n",
        "    if feature_extract:\n",
        "        return fc7\n",
        "\n",
        "    # fc8, 1000\n",
        "    fc8W = tf.Variable(net_data[\"fc8\"][0])\n",
        "    fc8b = tf.Variable(net_data[\"fc8\"][1])\n",
        "\n",
        "    logits = tf.matmul(fc7, fc8W) + fc8b\n",
        "    probabilities = tf.nn.softmax(logits)\n",
        "\n",
        "    return probabilities"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NFDbekpwMiAB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path\n",
        "sys.path.append('/content/CarND-Alexnet-Feature-Extraction/')\n",
        "sys.path.append('/content')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NlTQHgirMaYS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('train.p', 'rb') as f:\n",
        "    data = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rbGAoUswMeLz",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 11
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e9180d14-2a7a-45c5-80e0-4b13e9bba2fd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521546046816,
          "user_tz": -480,
          "elapsed": 14310,
          "user": {
            "displayName": "郝晓翼",
            "photoUrl": "//lh4.googleusercontent.com/-fyL4ZV1Be6g/AAAAAAAAAAI/AAAAAAAAAEw/TPv5-MRrZqA/s50-c-k-no/photo.jpg",
            "userId": "110086297311802405920"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "! wget https://www.cs.toronto.edu/~guerzhoy/tf_alexnet/bvlc_alexnet.npy"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-03-20 11:40:32--  https://www.cs.toronto.edu/~guerzhoy/tf_alexnet/bvlc_alexnet.npy\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 243861814 (233M)\n",
            "Saving to: ‘bvlc_alexnet.npy’\n",
            "\n",
            "bvlc_alexnet.npy    100%[===================>] 232.56M  13.5MB/s    in 11s     \n",
            "\n",
            "2018-03-20 11:40:44 (20.5 MB/s) - ‘bvlc_alexnet.npy’ saved [243861814/243861814]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hjgDpOumNfU6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "\n",
        "nb_classes = 43\n",
        "epochs = 10\n",
        "batch_size = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EQKQ6HT3OUVr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(data['features'], data['labels'], test_size=0.33, random_state=0)\n",
        "\n",
        "features = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
        "labels = tf.placeholder(tf.int64, None)\n",
        "resized = tf.image.resize_images(features, (227, 227))\n",
        "\n",
        "# Returns the second final layer of the AlexNet model,\n",
        "# this allows us to redo the last layer for the traffic signs\n",
        "# model.\n",
        "fc7 = AlexNet(resized, feature_extract=True)\n",
        "fc7 = tf.stop_gradient(fc7)\n",
        "shape = (fc7.get_shape().as_list()[-1], nb_classes)\n",
        "fc8W = tf.Variable(tf.truncated_normal(shape, stddev=1e-2))\n",
        "fc8b = tf.Variable(tf.zeros(nb_classes))\n",
        "logits = tf.nn.xw_plus_b(fc7, fc8W, fc8b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JVmIdos3PIKM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "7ef95aba-b60b-4c5b-9ae9-603c97da98fe",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521546537793,
          "user_tz": -480,
          "elapsed": 2267,
          "user": {
            "displayName": "郝晓翼",
            "photoUrl": "//lh4.googleusercontent.com/-fyL4ZV1Be6g/AAAAAAAAAAI/AAAAAAAAAEw/TPv5-MRrZqA/s50-c-k-no/photo.jpg",
            "userId": "110086297311802405920"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,logits=logits)\n",
        "loss_op = tf.reduce_mean(cross_entropy)\n",
        "opt = tf.train.AdamOptimizer()\n",
        "train_op = opt.minimize(loss_op, var_list=[fc8W, fc8b])\n",
        "init_op = tf.global_variables_initializer()\n",
        "\n",
        "preds = tf.arg_max(logits, 1)\n",
        "accuracy_op = tf.reduce_mean(tf.cast(tf.equal(preds, labels), tf.float32))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-29-7609321aa7d9>:7: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `argmax` instead\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yR9BX3D7PaIj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 10
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "e35ea9e1-6f2e-4ab0-b83c-3fcf27bdf80a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521547119386,
          "user_tz": -480,
          "elapsed": 550778,
          "user": {
            "displayName": "郝晓翼",
            "photoUrl": "//lh4.googleusercontent.com/-fyL4ZV1Be6g/AAAAAAAAAAI/AAAAAAAAAEw/TPv5-MRrZqA/s50-c-k-no/photo.jpg",
            "userId": "110086297311802405920"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def eval_on_data(X, y, sess):\n",
        "    total_acc = 0\n",
        "    total_loss = 0\n",
        "    for offset in range(0, X.shape[0], batch_size):\n",
        "        end = offset + batch_size\n",
        "        X_batch = X[offset:end]\n",
        "        y_batch = y[offset:end]\n",
        "\n",
        "        loss, acc = sess.run([loss_op, accuracy_op], feed_dict={features: X_batch, labels: y_batch})\n",
        "        total_loss += (loss * X_batch.shape[0])\n",
        "        total_acc += (acc * X_batch.shape[0])\n",
        "\n",
        "    return total_loss/X.shape[0], total_acc/X.shape[0]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init_op)\n",
        "\n",
        "    for i in range(epochs):\n",
        "        # training\n",
        "        X_train, y_train = shuffle(X_train, y_train)\n",
        "        t0 = time.time()\n",
        "        for offset in range(0, X_train.shape[0], batch_size):\n",
        "            end = offset + batch_size\n",
        "            sess.run(train_op, feed_dict={features: X_train[offset:end], labels: y_train[offset:end]})\n",
        "\n",
        "        val_loss, val_acc = eval_on_data(X_val, y_val, sess)\n",
        "        print(\"Epoch\", i+1)\n",
        "        print(\"Time: %.3f seconds\" % (time.time() - t0))\n",
        "        print(\"Validation Loss =\", val_loss)\n",
        "        print(\"Validation Accuracy =\", val_acc)\n",
        "        print(\"\")\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "Time: 55.429 seconds\n",
            "Validation Loss = 0.5188285538605555\n",
            "Validation Accuracy = 0.8618131231391933\n",
            "\n",
            "Epoch 2\n",
            "Time: 54.218 seconds\n",
            "Validation Loss = 0.3492542294213444\n",
            "Validation Accuracy = 0.9094211299126976\n",
            "\n",
            "Epoch 3\n",
            "Time: 54.312 seconds\n",
            "Validation Loss = 0.25805622893881874\n",
            "Validation Accuracy = 0.9395625627946518\n",
            "\n",
            "Epoch 4\n",
            "Time: 54.261 seconds\n",
            "Validation Loss = 0.22710421902290018\n",
            "Validation Accuracy = 0.9423448489294399\n",
            "\n",
            "Epoch 5\n",
            "Time: 54.231 seconds\n",
            "Validation Loss = 0.1913001163498365\n",
            "Validation Accuracy = 0.9526238503702292\n",
            "\n",
            "Epoch 6\n",
            "Time: 54.172 seconds\n",
            "Validation Loss = 0.17028443307773952\n",
            "Validation Accuracy = 0.9581111368730195\n",
            "\n",
            "Epoch 7\n",
            "Time: 54.220 seconds\n",
            "Validation Loss = 0.1575682178670596\n",
            "Validation Accuracy = 0.95787927969704\n",
            "\n",
            "Epoch 8\n",
            "Time: 54.294 seconds\n",
            "Validation Loss = 0.15002676732311276\n",
            "Validation Accuracy = 0.9575701367957338\n",
            "\n",
            "Epoch 9\n",
            "Time: 54.226 seconds\n",
            "Validation Loss = 0.1317190221278411\n",
            "Validation Accuracy = 0.9656078522527262\n",
            "\n",
            "Epoch 10\n",
            "Time: 54.223 seconds\n",
            "Validation Loss = 0.12842716509739024\n",
            "Validation Accuracy = 0.9660715665816524\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L-_HCZ4fPiNS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}